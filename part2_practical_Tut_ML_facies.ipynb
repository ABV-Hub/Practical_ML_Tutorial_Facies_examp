{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Machine Learning Tutorial, Multi-class Classification Problem: Geoscience example (Facies)\n",
    "\n",
    "[Ryan A. Mardani](https://www.linkedin.com/in/amardani/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will implement standard and comprehensive steps to select the best model and hyper prameters to predict rock facies form a dataset. First, we will prepare data for modeling, fit the models and cross validate, predict facies labels and evaluate prediction accuracy by several model evaluation metrics. Finally, we will examine model performance on blind-well data. These are the models that we will use:<br>\n",
    "1 - Logistic Regression Classifier <br>\n",
    "2 - K Neighbors Classifier<br>\n",
    "3 - Decision Tree Classifier<br>\n",
    "4 - Random Forest Classifier<br>\n",
    "5 - Support Vector Classifier<br>\n",
    "6 - Gaussian Naive Bayes Classifier<br>\n",
    "7 - Gradient Boosting Classifier<br>\n",
    "8 - Extra Tree Classifier<br>\n",
    "\n",
    "The dataset for this study comes from Hugoton and Panoma Fields in North America which was used as class exercise at The University of Kansas (Dubois et. al, 2007). It consists of log data(the measurement of physical properties of rocks) of nine wells. We will use these log data to train supervised classifiers in order to predict discrete facies groups. For more detail, you may take a look here. The seven features are:<br>\n",
    "\n",
    "__GR__: this wireline logging tools measure gamma emission<br>\n",
    "__ILD_log10__: this is resistivity measurement<br>\n",
    "__PE__: photoelectric effect log<br>\n",
    "__DeltaPHI__: Phi is a porosity index in petrophysics.<br>\n",
    "__PNHIND__: Average of neutron and density log.<br>\n",
    "__NM_M__:nonmarine-marine indicator<br>\n",
    "__RELPOS__: relative position<br>\n",
    "\n",
    "The nine discrete facies (classes of rocks) are:<br>\n",
    "(SS) Nonmarine sandstone<br>\n",
    "(CSiS) Nonmarine coarse siltstone<br>\n",
    "(FSiS) Nonmarine fine siltstone<br>\n",
    "(SiSH) Marine siltstone and shale<br>\n",
    "(MS) Mudstone (limestone)<br>\n",
    "(WS) Wackestone (limestone)<br>\n",
    "(D) Dolomite<br>\n",
    "(PS) Packstone-grainstone (limestone)<br>\n",
    "(BS) Phylloid-algal bafflestone (limestone)<br>\n",
    "For more detailand data explanation refer [here](https://github.com/mardani72/Facies-Classification-Machine-Learning/blob/master/Facies_Classification_Various_ML_Final.ipynb). <br>\n",
    "The project content:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1- Data Exploratory Analysis__\n",
    "> 1-1 Data visualization \n",
    ">> 1-1-1 log-plot<br>\n",
    ">> 1-1-2 Bar plot<br>\n",
    ">> 1-1-3 Cross-plot<br>\n",
    "\n",
    "> 1-2 Feature Engineering<br> \n",
    ">> 1-2-1 NaN imputation<br>\n",
    ">> 1-2-2 Feature extraction<br>\n",
    ">> 1-2-3 Oversampling<br>\n",
    "\n",
    "> 1-3 Feature Importance<br> \n",
    ">> 1-3-1 Feature linear corrolation<br>\n",
    ">> 1-3-2 Decision tree<br>\n",
    ">> 1-3-3 Permutation feature importance<br>\n",
    "\n",
    "#### __2- Build Model & Validate__\n",
    "> 2-1 Baseline Model<br> \n",
    "> 2-2 Hyper-parameters\n",
    ">> 2-2-1 Grid search<br>\n",
    "\n",
    "#### __3- Model Evaluation-1__\n",
    "> 3-1 Model Merices <br>\n",
    "> 3-2 Confusion matrix <br>\n",
    "#### __4- Model Evaluation-2__\n",
    "> 4-1 Learning curves<br> \n",
    "> 4-2 ROC plot<br> \n",
    "> 4-3 Blind well prediction and evaluation<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "pd.set_option('display.max_rows', 30)\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('facies_vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify some data types may python concern about\n",
    "df['Facies'] = df['Facies'].astype('int')\n",
    "df['Depth'] = df['Depth'].astype('float')\n",
    "df['Well Name'] = df['Well Name'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 Data visualization \n",
    "#### 1-1-1 log-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors \n",
    "facies_colors = ['xkcd:goldenrod', 'xkcd:orange','xkcd:sienna','xkcd:violet',\n",
    "       'xkcd:olive','xkcd:turquoise', \"xkcd:yellowgreen\", 'xkcd:indigo', 'xkcd:blue']\n",
    "\n",
    "facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', \n",
    "                 'MS',  'WS', 'D','PS', 'BS']\n",
    "#facies_color_map is a dictionary that maps facies labels to their respective colors\n",
    "facies_color_map = {}\n",
    "for ind, label in enumerate(facies_labels):\n",
    "    facies_color_map[label] = facies_colors[ind]\n",
    "\n",
    "def label_facies(row, labels):\n",
    "    return labels[ row['Facies'] -1]\n",
    "#establish facies label str    \n",
    "df.loc[:,'FaciesLabels'] = df.apply(lambda row: label_facies(row, facies_labels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is function to create a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_facies_log_plot(logs, facies_colors):\n",
    "    #make sure logs are sorted by depth\n",
    "    logs = logs.sort_values(by='Depth')\n",
    "    cmap_facies = colors.ListedColormap(\n",
    "            facies_colors[0:len(facies_colors)], 'indexed')\n",
    "    \n",
    "    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n",
    "    \n",
    "    cluster=np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)\n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=6, figsize=(12, 6))\n",
    "    ax[0].plot(logs.GR, logs.Depth, '-g',  alpha=0.8, lw = 0.9)\n",
    "    ax[1].plot(logs.ILD_log10, logs.Depth, '-b',  alpha=0.8, lw = 0.9)\n",
    "    ax[2].plot(logs.DeltaPHI, logs.Depth, '-k',  alpha=0.8, lw = 0.9)\n",
    "    ax[3].plot(logs.PHIND, logs.Depth, '-r',  alpha=0.8, lw = 0.9)\n",
    "    ax[4].plot(logs.PE, logs.Depth, '-c',  alpha=0.8, lw = 0.9)\n",
    "    im=ax[5].imshow(cluster, interpolation='none', aspect='auto',\n",
    "                    cmap=cmap_facies,vmin=1,vmax=9)\n",
    "    \n",
    "    divider = make_axes_locatable(ax[5])\n",
    "    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n",
    "    cbar=plt.colorbar(im, cax=cax)\n",
    "    cbar.set_label((5*' ').join([' SS ', 'CSiS', 'FSiS', \n",
    "                                'SiSh', ' MS ', ' WS ', ' D  ', \n",
    "                                ' PS ', ' BS ']))\n",
    "    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n",
    "    \n",
    "    for i in range(len(ax)-1):\n",
    "        ax[i].set_ylim(ztop,zbot)\n",
    "        ax[i].invert_yaxis()\n",
    "        ax[i].grid()\n",
    "        ax[i].locator_params(axis='x', nbins=3)\n",
    "    \n",
    "    ax[0].set_xlabel(\"GR\")\n",
    "    ax[0].set_xlim(logs.GR.min(),logs.GR.max())\n",
    "    ax[1].set_xlabel(\"ILD_log10\")\n",
    "    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())\n",
    "    ax[2].set_xlabel(\"DeltaPHI\")\n",
    "    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())\n",
    "    ax[3].set_xlabel(\"PHIND\")\n",
    "    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())\n",
    "    ax[4].set_xlabel(\"PE\")\n",
    "    ax[4].set_xlim(logs.PE.min(),logs.PE.max())\n",
    "    ax[5].set_xlabel('Facies')\n",
    "    \n",
    "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n",
    "    ax[4].set_yticklabels([]); ax[5].set_yticklabels([])\n",
    "    ax[5].set_xticklabels([])\n",
    "    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Well Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_facies_log_plot(\n",
    "    data[data['Well Name'] == 'SHRIMPLIN'],\n",
    "    facies_colors)\n",
    "# plt.savefig(\"Well_example.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1-2 Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's facies percentages first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = Counter(data.FaciesLabels)\n",
    "for i,j in cn.items():\n",
    "    percent = j / len(data) * 100\n",
    "    print('Class=%s, Count=%d, Percentage=%.3f%%' % (i, j, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(cn.keys(), cn.values(), color=facies_colors )\n",
    "plt.title('Facies Distribution')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.savefig(\"bar_plot.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is imbalanced dataset. Dolomite has the lowest member participation. Comparing coarse siltstone, dolomite appears 8 times less than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1-3 Cross-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.pairplot(data.drop(['Well Name','Facies','Formation','Depth','NM_M','RELPOS'],axis=1),\n",
    "             hue='FaciesLabels', palette=facies_color_map,\n",
    "             hue_order=list(reversed(facies_labels)))\n",
    "sns_plot.savefig('cross_plots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hereafter we will store dataset into new vriable after main operations (indented paragrpahs in introduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fe = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2 Feature Engineering \n",
    "##### 1-2-1 NaN imputation\n",
    "Let's look at the data to see if Null values are present in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fe.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find out which wells do not have PE\n",
    "df_null = data_fe.loc[data_fe.PE.isna()]\n",
    "df_null['Well Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are sevral way to deal with Null values in dataset. The simplest approach is to drop the rows containing at least one null value. This can be logical with bigger size dataset but with small sized dataframes single points are important. We can impute null values with mean or from adjecent data points in columns. Filling with mean value will not affect data variance and therefore will not have impact on pediction accuracy, though can create data bias. Another approach, which I will implement here, to employe machine learning models to predict missing values. This is the best way of dealing with this dataset becuase we have just single feature missing from dataset, PE. There is meaningful corrolation between this feature with others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fe.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use various ML models to predict PE log as countinious regression problem. Here, I will employ Multi-Layer Percepteron Neural Network from scikit-learn to predict target value. I am not going to deep for this approach and use simply to predict missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_PE = data_fe[['Facies','Depth', 'GR', 'ILD_log10',\n",
    "       'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']].dropna()  # select features and target log that has value\n",
    "X = set_PE[['Facies','Depth', 'GR', 'ILD_log10',\n",
    "       'DeltaPHI', 'PHIND', 'NM_M', 'RELPOS']]  # feature selection without null value\n",
    "XX = data_fe[['Facies','Depth', 'GR', 'ILD_log10',\n",
    "       'DeltaPHI', 'PHIND', 'NM_M', 'RELPOS']]\n",
    "y = set_PE['PE'] # target log\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_b = scaler.fit_transform(XX)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MLP_pe = MLPRegressor(random_state=1, max_iter= 500).fit(X_train, y_train) #fit the model\n",
    "MLP_pe.score(X_test, y_test) # examine accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fe['PE_pred'] = MLP_pe.predict(X_b)  # predict PE\n",
    "data_fe.PE.fillna(data_fe.PE_pred, inplace =True) # fill NaN vakues with predicted PE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot predecited PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_facies_log_plot(\n",
    "    data[data['Well Name'] == 'ALEXANDER D'],\n",
    "    facies_colors)\n",
    "plt.savefig(\"predicted_PE.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove predicted PE column\n",
    "data_fe = data_fe.drop(columns=['PE_pred'])\n",
    "data = data.drop(columns=['PE_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2-2 Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having limited set of features in this dataset can lead us to think about extracting some data from existing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can convert formation categorical data into numeric data. Our background knowledge can help us to predict that some facies are possibly presnet more in a specififc formation rather than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert formation category data into numeric to use as predictor, add 1 starting from 1\n",
    "data_fe['Formation_num'] = LabelEncoder().fit_transform(data_fe['Formation'].astype('str')) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fe['Well Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick up a well as blind: \n",
    "machine learning algorithm will not see this data in the training process. We will use it at the end to see how the model works. Remember to select a well that includes all types of facies classes, otherwise in prediction, data dimension inconsistency will generate an error. Or simply add a lacking facies example to the well to avoid the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind = data_fe[data_fe['Well Name'] == 'KIMZEY A']\n",
    "data_fe = data_fe[data_fe['Well Name'] != 'KIMZEY A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if new feature extraction would assisst prediction improvment, we should define a baseline model then compare with extracted feature model.\n",
    "##### Baseline Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_fe[['Depth', 'GR', 'ILD_log10','DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS', 'Formation_num']]\n",
    "y = data_fe['Facies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we will use logistic regression classifier as baseline model and will examine model performance with cross validation concept. Data will be splitted into 10 subgroups and process will be reapeted 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Accuracy: %.3f' % (mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can explore whether feature extraction can improve model performance. There are many aprroaches while we will use some transforms for chaining the distribution of the input variables such as Quantile Transformer and KBins Discretizer. Then, will remove linear dependecies between the input variables using PCA and TruncatedSVD.\n",
    "To study more refer [here](https://machinelearningmastery.com/quantile-transforms-for-machine-learning/).<br>\n",
    "Using feature union class, we will define list of transforms to perform results aggrigated together. This will create a dataset with lots of feature columns while we need to reduce dimentionality to faster and better performance. Finally, Recursive Feature Elimination, or RFE, technique can be used to select the most relevent features. We select 30 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "#-------------------------------------------------- append transforms into a list\n",
    "transforms = list()\n",
    "transforms.append(('qt', QuantileTransformer(n_quantiles=100, output_distribution='normal')))\n",
    "transforms.append(('kbd', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')))\n",
    "transforms.append(('pca', PCA(n_components=7)))\n",
    "transforms.append(('svd', TruncatedSVD(n_components=7)))\n",
    "#-------------------------------------------------- initialize the feature union\n",
    "fu = FeatureUnion(transforms)\n",
    "#-------------------------------------------------- define the feature selection\n",
    "rfe = RFE(estimator=LogisticRegression(solver='liblinear'), n_features_to_select=30)\n",
    "#-------------------------------------------------- define the model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "#-------------------------------------------------- use pipeline to chain operation\n",
    "steps = list()\n",
    "steps.append(('fu', fu))\n",
    "steps.append(('rfe', rfe))\n",
    "steps.append(('ml', model))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# define the cross-validation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f' % (mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy improvement shows that feature extraction can be useful approach when we are dealing with limited features in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2-3 Oversampeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In imbalanced datasets, we can use resampling technique to add some more data point to increase members of minority groups. This can be helpful whenever minority label targets has special importance such as credit card fraud detection. In that example, fraud can happen with less than 0.1 percent of transaction while it is important to detect fraud.<br>\n",
    "In this work, we will add psudo observation for Dolomite class which has the lowest poulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Synthetic Minority Oversampling Technique, SMOTE__: the technique is used to select nearest neighbors in the feature space, separated examples by adding a line and producing new examples along the line. The method is not merely generating the duplicates from the outnumbered class, but applied K-nearest neighbours to generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sm1 , y_sm1 = smote.fit_sample(X,y)\n",
    "X_sm , y_sm = X_sm1 , y_sm1  # keep for fuuture plotting an cimparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before SMOTE: \", Counter(y))\n",
    "print(\"After SMOTE: \", Counter(y_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, dataset is balanced. Let's see how works comparing Baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_sm = scaler.fit_transform(X_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bal = LogisticRegression(solver='liblinear')\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model_bal, X_sm, y_sm, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Accuracy: %.3f' % (mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy improved by 3 percent but in multi-class classification, accuracy is not the best evaluation metrics. We will cover others in next parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fi = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3 Feature Importance\n",
    "Some machine learning algorithms offer importance score to help the user to select the most efficient features for prediction.\n",
    "##### 1-3-1 Feature linear corrolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is simple: features have higher correlation coefficient with target values are important for prediction. We can extract these coef's like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression for feature importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "model = LinearRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.title('Logistic Regression Coefficients as Feature Importance Scores')\n",
    "pyplot.savefig('reg_importance.png')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-3-2 Decision tree\n",
    "This algorithm provides importance scores based on the reduction in the criterion used to split in each node such as entropy or Gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.title('Decision tree classifier Feature Importance Scores')\n",
    "pyplot.savefig('DTree.png')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a library that provides an efficient and effective implementation of the stochastic gradient boosting algorithm. This algorithm can be used with scikit-learn via the XGBRegressor and XGBClassifier classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.title('XGBoost classifier Feature Importance Scores')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-3-3 Permutation feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html) is a model inspection technique that can be used for any fitted estimator when the data is tabular. This is especially useful for non-linear or opaque estimators. The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# perform permutation importance\n",
    "results = permutation_importance(model, X, y, scoring='accuracy')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.title('Permutation Feature Importance Scores')\n",
    "pyplot.savefig('permu.png')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2__ Build Model & Validate\n",
    "#### 2-1 Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The philosophy of constructing a baseline model is simple: we need a basic and simple model to see how the adjustments on both data and model parameters can cause improvement in model performance. In fact, this is like a scale for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Classifiers\n",
    "log = LogisticRegression()  \n",
    "knn = KNeighborsClassifier()\n",
    "dtree = DecisionTreeClassifier()\n",
    "rtree = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "nb = GaussianNB()\n",
    "gbc = GradientBoostingClassifier()\n",
    "etree = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that uses pipeline to impelement data transformation and fit with model then cross validate\n",
    "def baseline_model(model_name):\n",
    "\n",
    "    model = model_name\n",
    "    steps = list()\n",
    "    steps.append(('ss', StandardScaler() ))\n",
    "    steps.append(('ml', model))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "     # balanced X,y from SMOTE can also be used \n",
    "    scores = cross_val_score(pipeline, X_sm, y_sm, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "    print(model,'Accuracy: %.3f' % (mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "baseline_model(log)\n",
    "baseline_model(knn)\n",
    "baseline_model(dtree)\n",
    "baseline_model(rtree)\n",
    "baseline_model(svm)\n",
    "baseline_model(nb)\n",
    "baseline_model(gbc)\n",
    "baseline_model(etree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2 Hyper-parameters\n",
    "##### 2-2-1 Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a baseline model in the previous section without adjucting hyper-parameters (parameters that are adjustable by user). Sometimes, careful selection if these parameters can improve model results noticeably. Grid search is designed to employe nymeric/string range for specific hyper parameter without haveing to code loop function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#logistic regression classifier\n",
    "\n",
    "#define hyper parameters and ranges\n",
    "param_grid_log = [{'C': [0.1, 1, 10], 'solver': ['lbfgs', 'liblinear'], \n",
    "                   'max_iter':[100, 300]}]\n",
    "#apply gridsearch\n",
    "grid_log  = GridSearchCV(log, param_grid=param_grid_log, cv=5)\n",
    "#fit model with grid search\n",
    "grid_log.fit(X_train, y_train)\n",
    "print('The best parameters for log classifier: ', grid_log.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#kNN classifier\n",
    "\n",
    "#define hyper parameters and ranges\n",
    "param_grid_knn = [{'n_neighbors': [2, 3, 4, 6, 8, 10], 'weights': [ 'uniform', 'distance'], \n",
    "                   'metric': ['euclidean', 'manhattan', 'minkowski']}]\n",
    "#apply gridsearch\n",
    "grid_knn  = GridSearchCV(knn, param_grid=param_grid_knn, cv=5)\n",
    "#fit model with grid search\n",
    "grid_knn.fit(X_train, y_train)\n",
    "print('The best parameters for knn classifier: ', grid_knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#decision tree classifier\n",
    "\n",
    "#define hyper parameters and ranges\n",
    "param_grid_dtree = [{'max_depth': [ 15, 20, 25, 30], 'criterion': ['gini',  'entropy']}]\n",
    "#apply gridsearch\n",
    "grid_dtree  = GridSearchCV(dtree, param_grid=param_grid_dtree, cv=5)\n",
    "#fit model with grid search\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "print('The best parameters for dtree classifier: ', grid_dtree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#random forest classifier\n",
    "\n",
    "#define hyper parameters and ranges\n",
    "param_grid_rtree = [{'max_depth': [5, 10, 15, 20], 'n_estimators':[100,300,500] ,\n",
    "                     'criterion': ['gini',  'entropy']}]\n",
    "#apply gridsearch\n",
    "grid_rtree  = GridSearchCV(rtree, param_grid=param_grid_rtree, cv=5)\n",
    "#fit model with grid search\n",
    "grid_rtree.fit(X_train, y_train)\n",
    "print('The best parameters for rtree classifier: ', grid_rtree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#SVM classifier\n",
    "\n",
    "#define hyper parameters and ranges\n",
    "param_grid_svm = [{'C': [100, 50, 10, 1.0, 0.1, 0.01], 'gamma': ['scale'], \n",
    "                   'kernel': ['poly', 'rbf', 'sigmoid'] }]\n",
    "#apply gridsearch\n",
    "grid_svm  = GridSearchCV(svm, param_grid=param_grid_svm, cv=5)\n",
    "#fit model with grid search\n",
    "grid_svm.fit(X_train, y_train)\n",
    "print('The best parameters for svm classifier: ', grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#gbc classifier\n",
    "\n",
    "#define hyper parameters and ranges\n",
    "param_grid_gbc = [{'learning_rate': [0.1, 1], 'n_estimators':[200,350,500]}]\n",
    "#apply gridsearch\n",
    "grid_gbc  = GridSearchCV(gbc, param_grid=param_grid_gbc, cv=5)\n",
    "#fit model with grid search\n",
    "grid_gbc.fit(X_train, y_train)\n",
    "print('The best parameters for gbc classifier: ', grid_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#etree classifier\n",
    "\n",
    "#define hyper parameters and ranges\n",
    "param_grid_etree = [{'max_depth': [15, 20, 25, 30, 35], 'n_estimators':[200,350,500] , \n",
    "                     'criterion': ['gini',  'entropy']}]\n",
    "#apply gridsearch\n",
    "grid_etree  = GridSearchCV(etree, param_grid=param_grid_etree, cv=5)\n",
    "#fit model with grid search\n",
    "grid_etree.fit(X_train, y_train)\n",
    "print('The best parameters for etree classifier: ', grid_etree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build model objects with optimized hyper-paramters and run baseline model with hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Classifiers with hyper params\n",
    "log_hp = LogisticRegression(C = 10, solver = 'lbfgs', max_iter= 200 ) \n",
    "knn_hp = KNeighborsClassifier(metric = 'manhattan', n_neighbors=2,weights= 'distance')\n",
    "dtree_hp = DecisionTreeClassifier(criterion = 'entropy', max_depth=20)\n",
    "rtree_hp = RandomForestClassifier(criterion='entropy', max_depth=20, n_estimators=500)\n",
    "svm_hp = SVC(C=100, gamma= 'scale', kernel='rbf'  )  \n",
    "nb_hp = GaussianNB()\n",
    "gbc_hp = GradientBoostingClassifier(learning_rate=0.1, n_estimators=500)\n",
    "etree_hp = ExtraTreesClassifier(criterion='gini', max_depth=35, n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "baseline_model(log_hp)\n",
    "baseline_model(knn_hp)\n",
    "baseline_model(dtree_hp)\n",
    "baseline_model(rtree_hp)\n",
    "baseline_model(svm_hp)\n",
    "baseline_model(nb_hp)\n",
    "baseline_model(gbc_hp)\n",
    "baseline_model(etree_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with baseline model performances, hype-parameters adjucment could helpe models performance improve. It seems that for ensemble models hyper-parameters are not as efficient as the rests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe to compare accuracy results\n",
    "dd = { 'model':['log' , 'knn',  'dtree', 'rtree', 'svm', 'nb', 'gbc', 'etree'],\n",
    "      'Baseline_model':[0.623, 0.880, 0.845, 0.910, 0.777, 0.357, 0.832, 0.934],\n",
    "      'Hyper_pram_model': [0.61, 0.925, 0.856, 0.920, 0.889, 0.338, 0.890, 0.932]}\n",
    "\n",
    "df_comp = pd.DataFrame(dd) \n",
    "round(df_comp,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ^^^^^^^^^^^^^^^^^^^^^^ END of PART 2 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) https://jakevdp.github.io/PythonDataScienceHandbook/index.html <br>\n",
    "2) https://scikit-learn.org/stable/supervised_learning.html#supervised-learning<br>\n",
    "3) https://machinelearningmastery.com/<br>\n",
    "4) [Brendon Hall Git](https://github.com/brendonhall/facies_classification/blob/master/Facies%20Classification%20-%20SVM.ipynb)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow2.2)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
